{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abakamousa/NER-Sangkak-challenge/blob/0.1/training/experimentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install librairies"
      ],
      "metadata": {
        "id": "Ib-uJVw2-Os8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall typing -y\n",
        "!pip install flair==0.9"
      ],
      "metadata": {
        "id": "ThK2HRmOagES",
        "outputId": "f4ab6674-3727-44a1-c84b-180a7bf95540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping typing as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair==0.9\n",
            "  Downloading flair-0.9-py3-none-any.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.3/319.3 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (2022.6.2)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (2.8.2)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (0.8.10)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (1.2.1)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 KB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (1.13.1+cu116)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (3.5.3)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (0.1.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (4.9.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (4.64.1)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from flair==0.9) (3.6.0)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==3.12.2->flair==0.9) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==3.12.2->flair==0.9) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==3.12.2->flair==0.9) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.2->flair==0.9) (1.22.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.4->flair==0.9) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim<=3.8.3,>=3.4.0->flair==0.9) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim<=3.8.3,>=3.4.0->flair==0.9) (1.10.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.1.1->flair==0.9) (4.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.1.1->flair==0.9) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.1.1->flair==0.9) (3.0)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->flair==0.9) (0.11.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair==0.9) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->flair==0.9) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.8,>=1.5.0->flair==0.9) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.0.0->flair==0.9) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->flair==0.9) (0.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.9) (3.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==3.12.2->flair==0.9) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==3.12.2->flair==0.9) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==3.12.2->flair==0.9) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==3.12.2->flair==0.9) (1.26.14)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from pymongo->hyperopt>=0.1.1->flair==0.9) (2.3.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==3.12.2->flair==0.9) (1.7.1)\n",
            "Building wheels for collected packages: gdown, mpld3, sqlitedict, langdetect, overrides\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=4b13f85e80111b78d2062ba600b9d9e14d23e785737834f1c00755d0d8482dd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/62/1e/926d1ebe7b1e733c78d627fd288d01b83feaf67efc06e0e4c3\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=2d6dc02dd764eaab19375a9aa1bbe6b14841a26f9cc9560726de0b4bb91c7549\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/9f/9d/d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16869 sha256=e561eb4cf6814d19d48c02b6170478299d30429760e0c8d5842679036630299e\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/c6/16/46e174009277f9bccdaa7215a243939d2f70180804b249bf3a\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=6324f329899737c3b5ca0b53183794c5483d3f8f46bc43d70d815e066bd4eff5\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=7fa73044afa56fc7ac876033b81aff6ad4e626cd622ac1430cd92d310d9ae613\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "Successfully built gdown mpld3 sqlitedict langdetect overrides\n",
            "Installing collected packages: tokenizers, sqlitedict, sentencepiece, overrides, mpld3, janome, segtok, more-itertools, langdetect, importlib-metadata, ftfy, deprecated, conllu, wikipedia-api, konoha, huggingface-hub, transformers, gdown, bpemb, flair\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 9.1.0\n",
            "    Uninstalling more-itertools-9.1.0:\n",
            "      Successfully uninstalled more-itertools-9.1.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.9 ftfy-6.1.1 gdown-3.12.2 huggingface-hub-0.12.1 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.1.0 tokenizers-0.13.2 transformers-4.26.1 wikipedia-api-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install flair\n",
        "#!pip uninstall flair\n"
      ],
      "metadata": {
        "id": "CErtxjRi9-9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import librairies"
      ],
      "metadata": {
        "id": "87-SFPnO-XWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_uBvk_eXDEyb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "\n",
        "import flair\n",
        "from typing import List\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/bbj/train.txt\", sep=\" \")\n",
        "df.tail(25)"
      ],
      "metadata": {
        "id": "3Zi6VU-miwdP",
        "outputId": "6c8f957f-a666-4ee2-a372-66ef3c4b4f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Msaʼnyə̂  O\n",
              "47213        ntʉ́  O\n",
              "47214        mnə́  O\n",
              "47215         nə́  O\n",
              "47216         yəŋ  O\n",
              "47217  ntwɔ̂kshwɛ  O\n",
              "47218         yə́  O\n",
              "47219          éé  O\n",
              "47220         bó  O\n",
              "47221       byáp  O\n",
              "47222        gaə́  O\n",
              "47223         pú  O\n",
              "47224      pútə́  O\n",
              "47225          é  O\n",
              "47226          lə  O\n",
              "47227           A  O\n",
              "47228        hwə́  O\n",
              "47229           m  O\n",
              "47230        pá'  O\n",
              "47231      cwəpuŋ  O\n",
              "47232          ě  O\n",
              "47233      phənyə  O\n",
              "47234         nə́  O\n",
              "47235          é  O\n",
              "47236          lə  O\n",
              "47237           .  O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbd48db1-27c5-45d9-94f2-11f7d1f1628a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Msaʼnyə̂</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47213</th>\n",
              "      <td>ntʉ́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47214</th>\n",
              "      <td>mnə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47215</th>\n",
              "      <td>nə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47216</th>\n",
              "      <td>yəŋ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47217</th>\n",
              "      <td>ntwɔ̂kshwɛ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47218</th>\n",
              "      <td>yə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47219</th>\n",
              "      <td>éé</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47220</th>\n",
              "      <td>bó</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47221</th>\n",
              "      <td>byáp</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47222</th>\n",
              "      <td>gaə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47223</th>\n",
              "      <td>pú</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47224</th>\n",
              "      <td>pútə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47225</th>\n",
              "      <td>é</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47226</th>\n",
              "      <td>lə</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47227</th>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47228</th>\n",
              "      <td>hwə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47229</th>\n",
              "      <td>m</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47230</th>\n",
              "      <td>pá'</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47231</th>\n",
              "      <td>cwəpuŋ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47232</th>\n",
              "      <td>ě</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47233</th>\n",
              "      <td>phənyə</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47234</th>\n",
              "      <td>nə́</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47235</th>\n",
              "      <td>é</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47236</th>\n",
              "      <td>lə</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47237</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbd48db1-27c5-45d9-94f2-11f7d1f1628a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbd48db1-27c5-45d9-94f2-11f7d1f1628a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbd48db1-27c5-45d9-94f2-11f7d1f1628a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define columns\n",
        "columns = {0 : 'text', 1 : 'ner'}\n",
        "# directory where the data resides\n",
        "data_folder = '/content/bbj/' \n",
        "# initializing the corpus\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file = 'train.txt',\n",
        "                              test_file = 'test.txt',\n",
        "                              dev_file = 'dev.txt')"
      ],
      "metadata": {
        "id": "t40jeM7i8V29",
        "outputId": "0981e710-a494-4d46-880f-74640a6cdcf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 08:13:56,001 Reading data from /content/bbj\n",
            "2023-03-04 08:13:56,009 Train: /content/bbj/train.txt\n",
            "2023-03-04 08:13:56,015 Dev: /content/bbj/dev.txt\n",
            "2023-03-04 08:13:56,017 Test: /content/bbj/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train size: \", len(corpus.train))\n",
        "print(\"Test size: \", len(corpus.test))\n",
        "print(\"Dev size: \", len(corpus.dev))"
      ],
      "metadata": {
        "id": "5qk71Vxq-tTv",
        "outputId": "8ff231bc-acfb-491b-dea2-c96ead5dee8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size:  3384\n",
            "Test size:  966\n",
            "Dev size:  483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "I0W6vvgmiKmP",
        "outputId": "34d0672f-5800-4b82-d4a3-3fc08d6bd8a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 3384 train + 483 dev + 966 test sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus.train[0].to_tagged_string('ner'))"
      ],
      "metadata": {
        "id": "6COt-lm7_DZp",
        "outputId": "c68a4ffd-7bdf-47d6-e904-a858c02c212f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Msaʼnyə̂ gɔtí cyətə nə́ bǎyá cyə́ nəjí pôʼ bǎhə́lə́\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_type = 'ner'\n",
        "\n",
        "#  make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
      ],
      "metadata": {
        "id": "GpF4syD4_foa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_types = [\n",
        "\n",
        "    WordEmbeddings('glove'),\n",
        "\n",
        "    # comment in this line to use character embeddings\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    # comment in these lines to use flair embeddings\n",
        "    # FlairEmbeddings('news-forward'),\n",
        "    # FlairEmbeddings('news-backward'),\n",
        "]"
      ],
      "metadata": {
        "id": "xNTq9RFddo7-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# initialize sequence tagger\n",
        "tagger = SequenceTagger(hidden_size=256,\n",
        "                        embeddings=embeddings,\n",
        "                        tag_dictionary=tag_dictionary,\n",
        "                        tag_type=tag_type,\n",
        "                        use_crf=True)\n",
        "\n",
        "# initialize trainer\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yAfBBZlCdnPm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_transf = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
        "                                       layers=\"-1\",\n",
        "                                       subtoken_pooling=\"first\",\n",
        "                                       fine_tune=True,\n",
        "                                       use_context=True,\n",
        "                                       )\n",
        "\n",
        "# initialize sequence tagger\n",
        "tagger_transf = SequenceTagger(hidden_size=256,\n",
        "                        embeddings=embeddings_transf,\n",
        "                        tag_dictionary=tag_dictionary,\n",
        "                        tag_type=tag_type,\n",
        "                        use_rnn=False,\n",
        "                        use_crf=False)\n",
        "\n",
        "trainer1 = ModelTrainer(tagger_transf, corpus)"
      ],
      "metadata": {
        "id": "c4HjYqwf8Sdw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training with ner-multi-fast'\n",
        "trainer.train('resources/taggers/ner-multi-fast',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=10)"
      ],
      "metadata": {
        "id": "WATGFau2uNfS",
        "outputId": "95828a1b-d75c-419d-e1df-9b96f687dec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 08:14:48,303 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,307 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings('glove')\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-03-04 08:14:48,309 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,315 Corpus: \"Corpus: 3384 train + 483 dev + 966 test sentences\"\n",
            "2023-03-04 08:14:48,317 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,319 Parameters:\n",
            "2023-03-04 08:14:48,321  - learning_rate: \"0.1\"\n",
            "2023-03-04 08:14:48,323  - mini_batch_size: \"32\"\n",
            "2023-03-04 08:14:48,327  - patience: \"3\"\n",
            "2023-03-04 08:14:48,328  - anneal_factor: \"0.5\"\n",
            "2023-03-04 08:14:48,329  - max_epochs: \"10\"\n",
            "2023-03-04 08:14:48,331  - shuffle: \"True\"\n",
            "2023-03-04 08:14:48,332  - train_with_dev: \"False\"\n",
            "2023-03-04 08:14:48,333  - batch_growth_annealing: \"False\"\n",
            "2023-03-04 08:14:48,336 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,337 Model training base path: \"resources/taggers/ner-multi-fast\"\n",
            "2023-03-04 08:14:48,338 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,340 Device: cpu\n",
            "2023-03-04 08:14:48,343 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:14:48,345 Embeddings storage mode: cpu\n",
            "2023-03-04 08:14:48,349 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/flair/trainers/trainer.py:76: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 08:14:50,967 epoch 1 - iter 10/106 - loss 1.79211555 - samples/sec: 123.46 - lr: 0.100000\n",
            "2023-03-04 08:14:53,173 epoch 1 - iter 20/106 - loss 1.11936842 - samples/sec: 145.47 - lr: 0.100000\n",
            "2023-03-04 08:14:54,886 epoch 1 - iter 30/106 - loss 0.87918186 - samples/sec: 187.24 - lr: 0.100000\n",
            "2023-03-04 08:14:56,652 epoch 1 - iter 40/106 - loss 0.79523462 - samples/sec: 181.65 - lr: 0.100000\n",
            "2023-03-04 08:14:58,675 epoch 1 - iter 50/106 - loss 0.78396207 - samples/sec: 158.73 - lr: 0.100000\n",
            "2023-03-04 08:15:01,138 epoch 1 - iter 60/106 - loss 0.71931472 - samples/sec: 130.55 - lr: 0.100000\n",
            "2023-03-04 08:15:02,895 epoch 1 - iter 70/106 - loss 0.67328444 - samples/sec: 182.93 - lr: 0.100000\n",
            "2023-03-04 08:15:04,616 epoch 1 - iter 80/106 - loss 0.63288763 - samples/sec: 186.39 - lr: 0.100000\n",
            "2023-03-04 08:15:06,298 epoch 1 - iter 90/106 - loss 0.60588112 - samples/sec: 190.63 - lr: 0.100000\n",
            "2023-03-04 08:15:08,380 epoch 1 - iter 100/106 - loss 0.57666689 - samples/sec: 154.09 - lr: 0.100000\n",
            "2023-03-04 08:15:09,591 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:15:09,594 EPOCH 1 done: loss 0.5571 - lr 0.1000000\n",
            "2023-03-04 08:15:10,826 DEV : loss 0.30960601568222046 - f1-score (micro avg)  0.0863\n",
            "2023-03-04 08:15:10,853 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:15:10,857 saving best model\n",
            "2023-03-04 08:15:23,460 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:15:25,335 epoch 2 - iter 10/106 - loss 0.38951023 - samples/sec: 171.10 - lr: 0.100000\n",
            "2023-03-04 08:15:27,480 epoch 2 - iter 20/106 - loss 0.35876722 - samples/sec: 149.53 - lr: 0.100000\n",
            "2023-03-04 08:15:29,986 epoch 2 - iter 30/106 - loss 0.35597889 - samples/sec: 128.19 - lr: 0.100000\n",
            "2023-03-04 08:15:32,164 epoch 2 - iter 40/106 - loss 0.34900162 - samples/sec: 147.50 - lr: 0.100000\n",
            "2023-03-04 08:15:33,853 epoch 2 - iter 50/106 - loss 0.35043615 - samples/sec: 189.90 - lr: 0.100000\n",
            "2023-03-04 08:15:35,788 epoch 2 - iter 60/106 - loss 0.34169621 - samples/sec: 165.76 - lr: 0.100000\n",
            "2023-03-04 08:15:37,463 epoch 2 - iter 70/106 - loss 0.33743978 - samples/sec: 191.53 - lr: 0.100000\n",
            "2023-03-04 08:15:39,117 epoch 2 - iter 80/106 - loss 0.33700836 - samples/sec: 193.97 - lr: 0.100000\n",
            "2023-03-04 08:15:40,928 epoch 2 - iter 90/106 - loss 0.33772433 - samples/sec: 177.18 - lr: 0.100000\n",
            "2023-03-04 08:15:43,120 epoch 2 - iter 100/106 - loss 0.33136872 - samples/sec: 146.29 - lr: 0.100000\n",
            "2023-03-04 08:15:44,510 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:15:44,515 EPOCH 2 done: loss 0.3349 - lr 0.1000000\n",
            "2023-03-04 08:15:46,181 DEV : loss 0.27669987082481384 - f1-score (micro avg)  0.1311\n",
            "2023-03-04 08:15:46,208 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:15:46,210 saving best model\n",
            "2023-03-04 08:15:52,023 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:15:53,989 epoch 3 - iter 10/106 - loss 0.31790735 - samples/sec: 163.77 - lr: 0.100000\n",
            "2023-03-04 08:15:55,704 epoch 3 - iter 20/106 - loss 0.30723587 - samples/sec: 187.03 - lr: 0.100000\n",
            "2023-03-04 08:15:57,969 epoch 3 - iter 30/106 - loss 0.31807908 - samples/sec: 141.58 - lr: 0.100000\n",
            "2023-03-04 08:16:00,399 epoch 3 - iter 40/106 - loss 0.31065781 - samples/sec: 132.02 - lr: 0.100000\n",
            "2023-03-04 08:16:02,170 epoch 3 - iter 50/106 - loss 0.31297885 - samples/sec: 182.24 - lr: 0.100000\n",
            "2023-03-04 08:16:03,878 epoch 3 - iter 60/106 - loss 0.30843918 - samples/sec: 187.68 - lr: 0.100000\n",
            "2023-03-04 08:16:05,502 epoch 3 - iter 70/106 - loss 0.30600904 - samples/sec: 198.13 - lr: 0.100000\n",
            "2023-03-04 08:16:07,045 epoch 3 - iter 80/106 - loss 0.30561681 - samples/sec: 207.90 - lr: 0.100000\n",
            "2023-03-04 08:16:08,606 epoch 3 - iter 90/106 - loss 0.30501256 - samples/sec: 205.55 - lr: 0.100000\n",
            "2023-03-04 08:16:10,166 epoch 3 - iter 100/106 - loss 0.30261139 - samples/sec: 205.58 - lr: 0.100000\n",
            "2023-03-04 08:16:11,318 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:16:11,321 EPOCH 3 done: loss 0.3019 - lr 0.1000000\n",
            "2023-03-04 08:16:13,160 DEV : loss 0.2541758120059967 - f1-score (micro avg)  0.1605\n",
            "2023-03-04 08:16:13,206 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:16:13,213 saving best model\n",
            "2023-03-04 08:16:19,992 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:16:21,815 epoch 4 - iter 10/106 - loss 0.29644453 - samples/sec: 176.19 - lr: 0.100000\n",
            "2023-03-04 08:16:23,491 epoch 4 - iter 20/106 - loss 0.29001765 - samples/sec: 191.41 - lr: 0.100000\n",
            "2023-03-04 08:16:25,123 epoch 4 - iter 30/106 - loss 0.30300240 - samples/sec: 196.56 - lr: 0.100000\n",
            "2023-03-04 08:16:27,479 epoch 4 - iter 40/106 - loss 0.29400454 - samples/sec: 136.07 - lr: 0.100000\n",
            "2023-03-04 08:16:30,074 epoch 4 - iter 50/106 - loss 0.29472449 - samples/sec: 123.65 - lr: 0.100000\n",
            "2023-03-04 08:16:31,854 epoch 4 - iter 60/106 - loss 0.29522666 - samples/sec: 180.32 - lr: 0.100000\n",
            "2023-03-04 08:16:33,687 epoch 4 - iter 70/106 - loss 0.29398800 - samples/sec: 174.99 - lr: 0.100000\n",
            "2023-03-04 08:16:35,408 epoch 4 - iter 80/106 - loss 0.29354298 - samples/sec: 186.70 - lr: 0.100000\n",
            "2023-03-04 08:16:37,087 epoch 4 - iter 90/106 - loss 0.29125819 - samples/sec: 191.16 - lr: 0.100000\n",
            "2023-03-04 08:16:38,755 epoch 4 - iter 100/106 - loss 0.29000428 - samples/sec: 192.39 - lr: 0.100000\n",
            "2023-03-04 08:16:39,741 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:16:39,744 EPOCH 4 done: loss 0.2905 - lr 0.1000000\n",
            "2023-03-04 08:16:41,127 DEV : loss 0.2462645024061203 - f1-score (micro avg)  0.1384\n",
            "2023-03-04 08:16:41,175 BAD EPOCHS (no improvement): 1\n",
            "2023-03-04 08:16:41,177 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:16:43,663 epoch 5 - iter 10/106 - loss 0.29779884 - samples/sec: 129.31 - lr: 0.100000\n",
            "2023-03-04 08:16:45,827 epoch 5 - iter 20/106 - loss 0.29421869 - samples/sec: 148.24 - lr: 0.100000\n",
            "2023-03-04 08:16:47,414 epoch 5 - iter 30/106 - loss 0.29665878 - samples/sec: 202.21 - lr: 0.100000\n",
            "2023-03-04 08:16:49,091 epoch 5 - iter 40/106 - loss 0.28757896 - samples/sec: 191.40 - lr: 0.100000\n",
            "2023-03-04 08:16:50,875 epoch 5 - iter 50/106 - loss 0.28817010 - samples/sec: 179.83 - lr: 0.100000\n",
            "2023-03-04 08:16:52,807 epoch 5 - iter 60/106 - loss 0.28542193 - samples/sec: 166.08 - lr: 0.100000\n",
            "2023-03-04 08:16:55,150 epoch 5 - iter 70/106 - loss 0.28593492 - samples/sec: 136.85 - lr: 0.100000\n",
            "2023-03-04 08:16:57,839 epoch 5 - iter 80/106 - loss 0.28493005 - samples/sec: 119.45 - lr: 0.100000\n",
            "2023-03-04 08:17:00,662 epoch 5 - iter 90/106 - loss 0.28132499 - samples/sec: 113.72 - lr: 0.100000\n",
            "2023-03-04 08:17:03,415 epoch 5 - iter 100/106 - loss 0.28047633 - samples/sec: 116.61 - lr: 0.100000\n",
            "2023-03-04 08:17:04,409 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:17:04,412 EPOCH 5 done: loss 0.2807 - lr 0.1000000\n",
            "2023-03-04 08:17:05,528 DEV : loss 0.23804204165935516 - f1-score (micro avg)  0.1659\n",
            "2023-03-04 08:17:05,554 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:17:05,556 saving best model\n",
            "2023-03-04 08:17:17,104 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:17:20,512 epoch 6 - iter 10/106 - loss 0.28233642 - samples/sec: 94.16 - lr: 0.100000\n",
            "2023-03-04 08:17:22,235 epoch 6 - iter 20/106 - loss 0.27866604 - samples/sec: 186.18 - lr: 0.100000\n",
            "2023-03-04 08:17:23,859 epoch 6 - iter 30/106 - loss 0.27982471 - samples/sec: 197.59 - lr: 0.100000\n",
            "2023-03-04 08:17:25,517 epoch 6 - iter 40/106 - loss 0.28320491 - samples/sec: 193.48 - lr: 0.100000\n",
            "2023-03-04 08:17:27,217 epoch 6 - iter 50/106 - loss 0.28263054 - samples/sec: 188.85 - lr: 0.100000\n",
            "2023-03-04 08:17:28,903 epoch 6 - iter 60/106 - loss 0.28098068 - samples/sec: 190.22 - lr: 0.100000\n",
            "2023-03-04 08:17:31,347 epoch 6 - iter 70/106 - loss 0.28088155 - samples/sec: 131.27 - lr: 0.100000\n",
            "2023-03-04 08:17:34,154 epoch 6 - iter 80/106 - loss 0.27598199 - samples/sec: 114.31 - lr: 0.100000\n",
            "2023-03-04 08:17:35,909 epoch 6 - iter 90/106 - loss 0.27270718 - samples/sec: 182.80 - lr: 0.100000\n",
            "2023-03-04 08:17:37,531 epoch 6 - iter 100/106 - loss 0.27660340 - samples/sec: 197.66 - lr: 0.100000\n",
            "2023-03-04 08:17:38,442 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:17:38,445 EPOCH 6 done: loss 0.2752 - lr 0.1000000\n",
            "2023-03-04 08:17:40,169 DEV : loss 0.23094868659973145 - f1-score (micro avg)  0.2236\n",
            "2023-03-04 08:17:40,195 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:17:40,198 saving best model\n",
            "2023-03-04 08:17:47,549 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:17:49,702 epoch 7 - iter 10/106 - loss 0.26885855 - samples/sec: 149.38 - lr: 0.100000\n",
            "2023-03-04 08:17:51,570 epoch 7 - iter 20/106 - loss 0.26053023 - samples/sec: 171.69 - lr: 0.100000\n",
            "2023-03-04 08:17:53,306 epoch 7 - iter 30/106 - loss 0.26956702 - samples/sec: 184.79 - lr: 0.100000\n",
            "2023-03-04 08:17:54,869 epoch 7 - iter 40/106 - loss 0.27716127 - samples/sec: 205.23 - lr: 0.100000\n",
            "2023-03-04 08:17:56,644 epoch 7 - iter 50/106 - loss 0.27324615 - samples/sec: 180.82 - lr: 0.100000\n",
            "2023-03-04 08:17:58,422 epoch 7 - iter 60/106 - loss 0.27249707 - samples/sec: 180.69 - lr: 0.100000\n",
            "2023-03-04 08:18:00,881 epoch 7 - iter 70/106 - loss 0.27365533 - samples/sec: 130.40 - lr: 0.100000\n",
            "2023-03-04 08:18:03,667 epoch 7 - iter 80/106 - loss 0.27416567 - samples/sec: 115.30 - lr: 0.100000\n",
            "2023-03-04 08:18:05,945 epoch 7 - iter 90/106 - loss 0.27183189 - samples/sec: 141.15 - lr: 0.100000\n",
            "2023-03-04 08:18:08,137 epoch 7 - iter 100/106 - loss 0.27234694 - samples/sec: 146.26 - lr: 0.100000\n",
            "2023-03-04 08:18:09,534 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:18:09,538 EPOCH 7 done: loss 0.2703 - lr 0.1000000\n",
            "2023-03-04 08:18:10,943 DEV : loss 0.22984454035758972 - f1-score (micro avg)  0.1956\n",
            "2023-03-04 08:18:10,974 BAD EPOCHS (no improvement): 1\n",
            "2023-03-04 08:18:10,976 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:18:13,188 epoch 8 - iter 10/106 - loss 0.28769477 - samples/sec: 145.27 - lr: 0.100000\n",
            "2023-03-04 08:18:15,771 epoch 8 - iter 20/106 - loss 0.28408504 - samples/sec: 124.17 - lr: 0.100000\n",
            "2023-03-04 08:18:18,540 epoch 8 - iter 30/106 - loss 0.27745545 - samples/sec: 115.83 - lr: 0.100000\n",
            "2023-03-04 08:18:20,965 epoch 8 - iter 40/106 - loss 0.27496970 - samples/sec: 132.27 - lr: 0.100000\n",
            "2023-03-04 08:18:23,129 epoch 8 - iter 50/106 - loss 0.26768980 - samples/sec: 148.16 - lr: 0.100000\n",
            "2023-03-04 08:18:24,946 epoch 8 - iter 60/106 - loss 0.26510368 - samples/sec: 176.88 - lr: 0.100000\n",
            "2023-03-04 08:18:26,781 epoch 8 - iter 70/106 - loss 0.26144114 - samples/sec: 174.86 - lr: 0.100000\n",
            "2023-03-04 08:18:28,890 epoch 8 - iter 80/106 - loss 0.26055419 - samples/sec: 152.08 - lr: 0.100000\n",
            "2023-03-04 08:18:31,067 epoch 8 - iter 90/106 - loss 0.26320501 - samples/sec: 147.36 - lr: 0.100000\n",
            "2023-03-04 08:18:33,534 epoch 8 - iter 100/106 - loss 0.26337492 - samples/sec: 130.02 - lr: 0.100000\n",
            "2023-03-04 08:18:34,953 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:18:34,955 EPOCH 8 done: loss 0.2650 - lr 0.1000000\n",
            "2023-03-04 08:18:36,044 DEV : loss 0.23338322341442108 - f1-score (micro avg)  0.1644\n",
            "2023-03-04 08:18:36,072 BAD EPOCHS (no improvement): 2\n",
            "2023-03-04 08:18:36,073 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:18:37,663 epoch 9 - iter 10/106 - loss 0.24402085 - samples/sec: 202.07 - lr: 0.100000\n",
            "2023-03-04 08:18:39,340 epoch 9 - iter 20/106 - loss 0.25444760 - samples/sec: 191.21 - lr: 0.100000\n",
            "2023-03-04 08:18:41,000 epoch 9 - iter 30/106 - loss 0.26965334 - samples/sec: 193.27 - lr: 0.100000\n",
            "2023-03-04 08:18:42,733 epoch 9 - iter 40/106 - loss 0.26667393 - samples/sec: 185.05 - lr: 0.100000\n",
            "2023-03-04 08:18:44,263 epoch 9 - iter 50/106 - loss 0.26666816 - samples/sec: 210.04 - lr: 0.100000\n",
            "2023-03-04 08:18:49,107 epoch 9 - iter 60/106 - loss 0.26833097 - samples/sec: 66.13 - lr: 0.100000\n",
            "2023-03-04 08:18:52,873 epoch 9 - iter 70/106 - loss 0.26992262 - samples/sec: 85.11 - lr: 0.100000\n",
            "2023-03-04 08:18:55,642 epoch 9 - iter 80/106 - loss 0.26690515 - samples/sec: 115.77 - lr: 0.100000\n",
            "2023-03-04 08:18:59,568 epoch 9 - iter 90/106 - loss 0.26593994 - samples/sec: 81.67 - lr: 0.100000\n",
            "2023-03-04 08:19:06,612 epoch 9 - iter 100/106 - loss 0.26375518 - samples/sec: 45.55 - lr: 0.100000\n",
            "2023-03-04 08:19:08,972 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:19:08,987 EPOCH 9 done: loss 0.2625 - lr 0.1000000\n",
            "2023-03-04 08:19:11,628 DEV : loss 0.24657273292541504 - f1-score (micro avg)  0.2241\n",
            "2023-03-04 08:19:11,673 BAD EPOCHS (no improvement): 0\n",
            "2023-03-04 08:19:11,681 saving best model\n",
            "2023-03-04 08:19:20,308 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:19:22,144 epoch 10 - iter 10/106 - loss 0.27013788 - samples/sec: 175.26 - lr: 0.100000\n",
            "2023-03-04 08:19:23,788 epoch 10 - iter 20/106 - loss 0.26117098 - samples/sec: 195.07 - lr: 0.100000\n",
            "2023-03-04 08:19:25,494 epoch 10 - iter 30/106 - loss 0.26845457 - samples/sec: 188.00 - lr: 0.100000\n",
            "2023-03-04 08:19:27,194 epoch 10 - iter 40/106 - loss 0.25661853 - samples/sec: 188.58 - lr: 0.100000\n",
            "2023-03-04 08:19:28,813 epoch 10 - iter 50/106 - loss 0.25754772 - samples/sec: 198.18 - lr: 0.100000\n",
            "2023-03-04 08:19:30,532 epoch 10 - iter 60/106 - loss 0.25572147 - samples/sec: 186.60 - lr: 0.100000\n",
            "2023-03-04 08:19:32,913 epoch 10 - iter 70/106 - loss 0.26193484 - samples/sec: 134.91 - lr: 0.100000\n",
            "2023-03-04 08:19:35,618 epoch 10 - iter 80/106 - loss 0.26170941 - samples/sec: 118.83 - lr: 0.100000\n",
            "2023-03-04 08:19:38,387 epoch 10 - iter 90/106 - loss 0.26290810 - samples/sec: 115.98 - lr: 0.100000\n",
            "2023-03-04 08:19:40,777 epoch 10 - iter 100/106 - loss 0.26107736 - samples/sec: 134.50 - lr: 0.100000\n",
            "2023-03-04 08:19:41,750 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:19:41,752 EPOCH 10 done: loss 0.2601 - lr 0.1000000\n",
            "2023-03-04 08:19:42,871 DEV : loss 0.2278469204902649 - f1-score (micro avg)  0.1881\n",
            "2023-03-04 08:19:42,899 BAD EPOCHS (no improvement): 1\n",
            "2023-03-04 08:19:49,936 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:19:49,946 loading file resources/taggers/ner-multi-fast/best-model.pt\n",
            "2023-03-04 08:19:54,304 0.5603\t0.2386\t0.3347\t0.2016\n",
            "2023-03-04 08:19:54,308 \n",
            "Results:\n",
            "- F-score (micro) 0.3347\n",
            "- F-score (macro) 0.2691\n",
            "- Accuracy 0.2016\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         PER     0.6007    0.4261    0.4985       399\n",
            "        DATE     0.5143    0.3000    0.3789       180\n",
            "         LOC     0.9091    0.0781    0.1439       256\n",
            "         ORG     0.1842    0.0323    0.0549       217\n",
            "\n",
            "   micro avg     0.5603    0.2386    0.3347      1052\n",
            "   macro avg     0.5521    0.2091    0.2691      1052\n",
            "weighted avg     0.5751    0.2386    0.3003      1052\n",
            " samples avg     0.2016    0.2016    0.2016      1052\n",
            "\n",
            "2023-03-04 08:19:54,311 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.33466666666666667,\n",
              " 'dev_score_history': [0.08633093525179858,\n",
              "  0.13106796116504857,\n",
              "  0.16052060737527116,\n",
              "  0.13842482100238662,\n",
              "  0.16591928251121077,\n",
              "  0.22362869198312238,\n",
              "  0.19555555555555554,\n",
              "  0.16444444444444445,\n",
              "  0.2241014799154334,\n",
              "  0.18807339449541285],\n",
              " 'train_loss_history': [0.5571249644069259,\n",
              "  0.3349133215764351,\n",
              "  0.30188762453463974,\n",
              "  0.29052446473929033,\n",
              "  0.2806757074571407,\n",
              "  0.2751865146722319,\n",
              "  0.27033678673095934,\n",
              "  0.2649503995488027,\n",
              "  0.26247149711564793,\n",
              "  0.26014722272027885],\n",
              " 'dev_loss_history': [tensor(0.3096),\n",
              "  tensor(0.2767),\n",
              "  tensor(0.2542),\n",
              "  tensor(0.2463),\n",
              "  tensor(0.2380),\n",
              "  tensor(0.2309),\n",
              "  tensor(0.2298),\n",
              "  tensor(0.2334),\n",
              "  tensor(0.2466),\n",
              "  tensor(0.2278)]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start training with xlm-roberta-large-masakhaner\n",
        "\n",
        "trainer1.train('resources/taggers/sota-ner-flert', #xlm-roberta-large-masakhaner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=4,\n",
        "              mini_batch_chunk_size=1,\n",
        "              max_epochs=2)"
      ],
      "metadata": {
        "id": "fzON_Lt0qRtH",
        "outputId": "c8256477-3a3c-4f2f-e4be-3c2c169c3f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 08:29:13,761 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,779 Model: \"SequenceTagger(\n",
            "  (embeddings): TransformerWordEmbeddings(\n",
            "    (model): XLMRobertaModel(\n",
            "      (embeddings): XLMRobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 1024)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): XLMRobertaEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (18): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (19): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (20): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (21): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (22): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (23): XLMRobertaLayer(\n",
            "            (attention): XLMRobertaAttention(\n",
            "              (self): XLMRobertaSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): XLMRobertaSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): XLMRobertaIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): XLMRobertaOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): XLMRobertaPooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (linear): Linear(in_features=1024, out_features=11, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2023-03-04 08:29:13,782 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,788 Corpus: \"Corpus: 3384 train + 483 dev + 966 test sentences\"\n",
            "2023-03-04 08:29:13,790 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,794 Parameters:\n",
            "2023-03-04 08:29:13,796  - learning_rate: \"0.1\"\n",
            "2023-03-04 08:29:13,797  - mini_batch_size: \"4\"\n",
            "2023-03-04 08:29:13,801  - patience: \"3\"\n",
            "2023-03-04 08:29:13,802  - anneal_factor: \"0.5\"\n",
            "2023-03-04 08:29:13,805  - max_epochs: \"2\"\n",
            "2023-03-04 08:29:13,807  - shuffle: \"True\"\n",
            "2023-03-04 08:29:13,808  - train_with_dev: \"False\"\n",
            "2023-03-04 08:29:13,809  - batch_growth_annealing: \"False\"\n",
            "2023-03-04 08:29:13,810 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,812 Model training base path: \"resources/taggers/sota-ner-flert\"\n",
            "2023-03-04 08:29:13,813 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,814 Device: cpu\n",
            "2023-03-04 08:29:13,815 ----------------------------------------------------------------------------------------------------\n",
            "2023-03-04 08:29:13,816 Embeddings storage mode: cpu\n",
            "2023-03-04 08:29:13,830 ----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}